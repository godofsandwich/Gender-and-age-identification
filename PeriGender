### The Residual block consists of :
#  Conv_layer_1 ---> Batch_Normalization_1 ---> ReLU_activation_function_1 
#  ---> Conv_layer_2 ---> Batch_Normalization_2 ---> ReLU_activation_function_2 

class ResBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
    
    def forward(self, x):
        
        #print("\nInput shape for resblock : ",x.shape)
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        #print("ResBlock output shape:", out.shape)
        
        return out


#  The skip connection function consists of simply 2 layers :
#  Conv_layer_1 ---> MaxPool_layer_1
 
class SkipConnection1 (nn.Module):
    def __init__(self, in_channels, out_channels):
        super(SkipConnection1, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels=3, kernel_size=3, stride=1, padding=0)
        self.pool = nn.MaxPool2d(kernel_size=8, stride=8, padding=1)
        
    def forward(self, x):
        #print("Skip connection input shape : ",x.shape)
        out = self.conv(x)
        out = self.pool(out)
        #print("Skip Connection output shape:", out.shape)
        return out
    
class SkipConnection2 (nn.Module):
    def __init__(self, in_channels, out_channels):
        super(SkipConnection2, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels=3, kernel_size=1, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=5, stride=4, padding=0)
        
    def forward(self, x):
        out = self.conv(x)
        out = self.pool(out)
        return out

class SkipConnection3(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(SkipConnection3, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels=3, kernel_size=1, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=4, stride=2, padding=0)

    def forward(self, x):
        out = self.conv(x)
        out = self.pool(out)
        return out


class SkipConnection4(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(SkipConnection4, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels=3, kernel_size=1, stride=1, padding=0)
        self.pool = nn.MaxPool2d(kernel_size=(1,2), stride=(1,1))
        
    def forward(self, x):
        out = self.conv(x)
        out = self.pool(out)
        return out


    
#   The Perigender Layer is a combination of the aforementioned function in the following fashion : 
#    
#      1. Conv_layer_1 --> MaxPool2d_1 
#      2. Skip_function_1
#      3. Residual_batch_1             
#      4. Skip_function_2
#      5. Residual_batch_2
#      6. Skip_function_3
#      7. Residual_function_3
#      8. Skip_function_4
#      9. Residual_function_4  
#     10. MaxPool2d_2
#     11. Concatenation_layer_1
#     12. AvgPool_layer


class PeriGender(nn.Module):
    def __init__(self, num_classes=2):
        super(PeriGender, self).__init__()

        self.conv = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=2,padding=1)
        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)
        
        self.skip1 = SkipConnection1(64, 64)
        self.resblock1 = nn.Sequential(ResBlock(64, 64, stride=2), ResBlock(64, 128))
        
        self.skip2 = SkipConnection2(128, 128)
        self.resblock2 = nn.Sequential(ResBlock(128, 128, stride=2), ResBlock(128, 256))
        
        self.skip3 = SkipConnection3(256, 256)
        self.resblock3 = nn.Sequential(ResBlock(256, 256, stride=2), ResBlock(256, 512))
        
        self.skip4 = SkipConnection4(512, 4)
        self.resblock4 = nn.Sequential(ResBlock(512, 512, stride=2), ResBlock(512, 512))
        
        self.maxpool2 = nn.MaxPool2d(kernel_size=(1,2), stride=(1,1))
        
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.softmax = nn.Softmax(dim=1)
        
        self.fc = nn.Linear(524, num_classes)

        

    def forward(self, x):
        
        #print("Input shape : ",x.shape)
   
        out = self.conv(x)
        #print("shape after conv : ",out.shape)
        out = self.maxpool1(out)
        #print("shape after maxpool : ",out.shape)
        skip1 = self.skip1(out)
        #print("Shape after skip 1 : ",skip1.shape)
        out = self.resblock1(out)
        #print("Shape after residual group 1 : ",out.shape)
        
        skip2 = self.skip2(out)
        out = self.resblock2(out)
        #print("Shape after residual group 2: ",out.shape)
        
        skip3 = self.skip3(out)
        out = self.resblock3(out)
        #print("Shape after residual group 3 : ",out.shape)
        
        skip4 = self.skip4(out)
        #out = self.resblock4(out)
        #print("Shape after residual group 4: ",out.shape)
        
        out = self.maxpool2(out)
        #print("final output Shape after maxpool : ",out.shape)
                
        #print("Size of skip1 : ",skip1.shape)
        #print("Size of skip2 : ",skip2.shape)
        #print("Size of skip3 : ",skip3.shape)
        #print("Size of skip4 : ",skip4.shape)
        
        skip_outputs = [skip1, skip2, skip3, skip4,out]
        concatenated = torch.cat(skip_outputs, dim=1)
        
        #print("Shape of image after concatenation : ",concatenated.shape)
        out = self.avg_pool(concatenated)
        #print("Shape of image after avgpool : ",out.shape)
        out = out.squeeze()
        out = self.fc(out)
        #print("Shape after fc layer : ",out)
        
        return out  

